# NLP Math Foundation

This project explores the mathematical foundations behind Natural Language Processing (NLP), starting from linear algebra.  
Each concept is explained through interactive notebooks with code, visualizations, and NLP applications.

## ðŸ“š Structure

- `notebooks/` â€“ Jupyter Notebooks with explanations and visual demos
- `src/` â€“ Clean Python implementations of core math functions
- `examples/` â€“ Mini-scripts demonstrating practical usage

## âœ… Topics Covered (Planned)

1. Vectors and Basic Operations  
2. Dot Product & Cosine Similarity  
3. Matrices and Multiplications  
4. Linear Transformations  
5. Eigenvalues & Eigenvectors  
6. SVD and Latent Semantic Analysis  
7. Distance Metrics & Projections  

## ðŸ§  Why?

NLP models like Word2Vec, BERT, and Transformers are built upon linear algebra operations. This project helps you (and others) understand them from the ground up.

## ðŸ”§ Requirements

```bash
pip install numpy matplotlib
